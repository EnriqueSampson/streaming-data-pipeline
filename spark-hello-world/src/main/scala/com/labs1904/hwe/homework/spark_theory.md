# Overview

Similar to the work you did for Kafka, this is your crash course into Spark through different questions. In this homework, your
challenge is to write answers that make sense to you, and most importantly, **in your own words!**
Two of the best skills you can get from this class are to find answers to your questions using any means possible, and to
reword confusing descriptions in a way that makes sense to you. 

### Tips
* You don't need to write novels, just write enough that you feel like you've fully answered the question
* Use the helpful resources that we post next to the questions as a starting point, but carve your own path by searching on Google, YouTube, books in a library, etc to get answers!
* We're here if you need us. Reach out anytime if you want to ask deeper questions about a topic 
* This file is a markdown file. We don't expect you to do any fancy markdown, but you're welcome to format however you like
* Spark By Examples is a great resources to start with - [Spark By Examples](https://sparkbyexamples.com/)

### Your Challenge
1. Create a new branch for your answers 
2. Complete all of the questions below by writing your answers under each question
3. Commit your changes and push to your forked repository

## Questions
#### What problem does Spark help solve? Use a specific use case in your answer 
* Helpful resource: [Apache Spark Use Cases](https://www.toptal.com/spark/introduction-to-apache-spark)
* [Overivew of Apache Spark](https://www.youtube.com/watch?v=znBa13Earms&t=42s)
* Spark helps solve large scale data processing. It provides a unified solution for big data processing tasks, such as batch processing, real-time stream processing, and iterative machine learning.
* One specific use case where Spark excels is in processing and analyzing data generated by IoT (Internet of Things) devices. In this context, Spark helps manage the vast amounts of data generated by numerous sensors and devices in real-time, enabling businesses to gain actionable insights, detect anomalies, and make informed decisions. By leveraging Spark's in-memory processing capabilities and fault-tolerant architecture, organizations can process massive volumes of IoT data at scale, significantly improving their operational efficiency and competitive advantage.

#### What is Apache Spark?
* Helpful resource: [Spark Overview](https://www.youtube.com/watch?v=ymtq8yjmD9I) 
* Apache spark is an open source distributed computing system. 
* Mainly its in memory data processing which is faster than disk based processing.

#### What is distributed data processing? How does it relate to Apache Spark?  
[Apache Spark for Beginners](https://medium.com/@aristo_alex/apache-spark-for-beginners-d3b3791e259e)
* Distributed data processing refers to the practice of dividing and distributing large volumes of data across multiple interconnected computer nodes, or servers, to process the data simultaneously and more efficiently. This approach enables the parallel execution of tasks, allowing for faster data processing and analysis, especially when dealing with massive datasets that cannot be effectively handled by a single machine. Distributed processing leverages the combined computational power, memory, and storage resources of the connected nodes to tackle complex problems, provide fault tolerance, and enable scalability.
* in summary dividing and sending large data to multiple computer nodes for processing.
* Apache Spark is an example of a distributed data processing system. It is specifically designed to manage and process large-scale data by dividing it into smaller chunks called partitions. These partitions are distributed across multiple nodes in a cluster, and Spark processes the data in parallel, taking advantage of the combined computational resources of the cluster. Spark's in-memory processing capability, fault-tolerant architecture, and built-in libraries for various data processing tasks make it a powerful tool for distributed data processing, enabling organizations to efficiently analyze and process massive amounts of data in a timely manner.
* in summary spark is a system for distributed data processing, basically its designed to split things up into parititons for processing. it also process in parallel and in mem processing.



#### On the physical side of a spark cluster, you have a driver and executors. Define each and give an example of how they work together to process data
* Driver: The driver program is the central coordinator of the Spark application. It runs the main function, defines one or more SparkContexts, and translates the high-level operations (e.g., transformations and actions) into a series of tasks that can be executed on the cluster. The driver also coordinates with the cluster manager to allocate resources for the application, manages the application's lifecycle, and monitors the progress of tasks. Additionally, it is responsible for aggregating the results from the executors and returning the final output to the user.
* in summary, the driver is basically spins up the spark contexts and distrbutes to the executors. it also translates the high level ops like transforms and actions into a series of tasks for the executors. it also helps manage and allocate resources. 
* Executors: Executors are worker nodes within the Spark cluster that run the tasks assigned by the driver. Each executor has a dedicated JVM and runs multiple tasks concurrently in separate threads. Executors are responsible for processing the data, storing the data in their memory, and communicating the results back to the driver. They also maintain the intermediate data for fault tolerance, allowing for the recovery of lost data in case of a node failure.
* the actual processors of data and sends back to driver
* Example: Let's say you have a Spark application that processes a large dataset to compute the average temperature per city from a collection of weather data. In this scenario, the driver program would split the input data into smaller partitions, assign tasks to process each partition, and distribute these tasks to the executors in the cluster. The executors would then process their respective partitions in parallel, calculating the local average temperatures for their assigned cities. Once all tasks are complete, the driver would aggregate the results from the executors, compute the final average temperatures for each city, and return the output to the user.

#### Define each and explain how they are different from each other 
* RDD (Resilient Distributed Dataset)
* RDD is the fundamental data structure of Apache Spark, introduced in its early versions. It is an immutable, distributed collection of objects that can be processed in parallel across a cluster. RDDs are fault-tolerant, meaning that they can automatically recover from node failures. They provide low-level, fine-grained control over data partitioning and transformations, allowing developers to optimize their applications for performance. RDD operations are mainly divided into transformations (e.g., map, filter) and actions (e.g., count, reduce). RDDs are created either by parallelizing existing collections or by loading data from external storage systems.
* DataFrame
* Introduced in Spark 1.3, DataFrames are an abstraction built on top of RDDs, providing a higher-level, more expressive API for data manipulation. They represent structured data as a collection of named columns, similar to a table in a relational database. DataFrames can be created from a variety of data sources, including Hive, Avro, Parquet, ORC, JSON, and JDBC. They offer support for schema inference, allowing Spark to automatically determine the schema based on the data. DataFrames also enable optimizations through the Catalyst query optimizer and the Tungsten execution engine, resulting in improved performance compared to RDDs. However, they lack the strong static typing offered by RDDs.
* DataSet
* Introduced in Spark 1.6, DataSets are an extension of DataFrames that combine the best of both RDDs and DataFrames. They provide the strong static typing and functional programming capabilities of RDDs while retaining the performance optimizations and convenient API of DataFrames. DataSets represent a distributed collection of data that offers the benefits of both compile-time type safety and runtime optimizations through the Catalyst optimizer and Tungsten execution engine. They enable developers to perform powerful, type-safe data manipulation while still leveraging the performance advantages of Spark's optimized execution engine.
* In summary, RDDs are the foundational data structure in Spark, providing low-level control and fault tolerance. DataFrames are a higher-level abstraction with a convenient API for structured data and query optimizations. DataSets combine the benefits of both RDDs and DataFrames, offering strong static typing, functional programming capabilities, and performance optimizations.

#### What is a spark transformation?
[Spark By Examples-Transformations](https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-transformations/)
* A Spark transformation is an operation applied to an RDD (Resilient Distributed Dataset), DataFrame, or DataSet in Apache Spark that produces a new, transformed dataset. Transformations are lazily evaluated, meaning they are not executed immediately when called. Instead, Spark records the transformations and only executes them when an action is called, which requires the transformed data to be computed.
* Transformations can be classified as narrow or wide:
* Narrow transformations: These transformations do not require shuffling of data between partitions. Examples include map, filter, and union. In these cases, the data in the input partition can be directly used to produce the output partition without any data exchange between partitions.
* Wide transformations: These transformations require shuffling of data between partitions, which can be computationally expensive and may involve network overhead. Examples include groupByKey, reduceByKey, and join. Wide transformations can cause the data to be redistributed across the cluster, leading to the creation of new partitions and data processing stages.
* Some common Spark transformations include:
* map: Applies a function to each element in the dataset and returns a new dataset with the results.
* filter: Returns a new dataset containing only the elements that satisfy a given predicate function.
* flatMap: Applies a function to each element in the dataset and returns a new dataset formed by concatenating the resulting collections.
* groupByKey: Groups the elements in the dataset by key, producing a new dataset of (key, values) pairs.
* reduceByKey: Groups the elements in the dataset by key and applies a reduce function to the values of each group, resulting in a new dataset of (key, reduced value) pairs.
* Transformations allow you to build a series of data processing steps, creating a logical execution plan for your Spark application. This approach enables Spark to optimize the execution by rearranging or combining transformations when necessary, improving the overall performance of your application.


#### What is a spark action? How do actions differ from transformations? 
* A Spark action is an operation applied to an RDD (Resilient Distributed Dataset), DataFrame, or DataSet in Apache Spark that triggers the computation and returns a value to the driver program or writes data to an external storage system. Actions are the operations that force the execution of the transformations that have been lazily recorded by Spark, causing the data to be processed and the results to be computed.
* Actions differ from transformations in the following ways:
* Evaluation: Actions are eagerly evaluated, meaning they trigger the execution of the transformations immediately, whereas transformations are lazily evaluated and only recorded until an action is called.
* Output: Actions typically return a value to the driver program or write data to an external storage system, whereas transformations create a new, transformed dataset.
* Execution: Actions cause the execution plan to be optimized and materialized, while transformations just build the logical plan for future execution.
* Purpose: Actions are used to obtain results, compute aggregates, or persist data, while transformations are used to manipulate and process the data in the dataset.
* Some common Spark actions include:
* count: Returns the number of elements in the dataset.
* collect: Retrieves all elements from the dataset and returns them as an array to the driver program. This action should be used with caution on large datasets, as it may cause the driver to run out of memory.
* take: Returns the first n elements from the dataset as an array.
* reduce: Applies a binary function to the elements of the dataset, successively combining elements in pairs to produce a single value.
* saveAsTextFile: Writes the elements of the dataset to a text file in a specified directory, with each element being written as a separate line.
* In summary, actions trigger the execution of transformations, return a value to the driver program or write data to external storage, and are eagerly evaluated. Transformations, on the other hand, manipulate the data and create a new dataset without immediate execution, as they are lazily evaluated.
* 

#### What is a partition in spark? Why would you ever need to repartition? 
[Spark Partitioning](https://sparkbyexamples.com/spark/spark-repartition-vs-coalesce/)
* A partition in Spark is a logical chunk of data that represents a portion of the distributed dataset stored across the nodes in a Spark cluster. Partitions are the fundamental units of parallelism in Spark, as they enable data to be processed independently and concurrently by multiple executors. Each partition is processed on a single node, and the tasks associated with the partitions are distributed across the cluster. By dividing a large dataset into smaller partitions, Spark can efficiently parallelize the processing workload, leading to faster execution times and better resource utilization.
* Repartitioning may be necessary in certain situations to optimize the performance of a Spark application, such as:
* Load balancing: If the data is not evenly distributed across the partitions, it can lead to some nodes being overloaded with data, while others remain underutilized. Repartitioning can help redistribute the data more evenly, ensuring better load balancing and resource utilization.
* Changing the level of parallelism: You might need to increase or decrease the number of partitions based on the resources available in your cluster or the specific requirements of your application. Adjusting the number of partitions can help achieve better parallelism and improve the performance of your Spark application.
* Reducing network overhead: Certain operations, such as joins or groupBy, may cause data shuffling across the network. Repartitioning can help minimize network overhead by organizing the data in a way that reduces the amount of data that needs to be transferred between nodes.
* Optimizing partition sizes: Having partitions that are too small or too large can negatively impact performance. Small partitions can result in higher task scheduling overhead, while large partitions may cause out-of-memory errors or slow down processing times. Repartitioning can help create partitions with optimal sizes, improving the overall performance of your application.
* To repartition your data in Spark, you can use methods like repartition or coalesce. The repartition method shuffles the data and creates a new set of partitions, while the coalesce method reduces the number of partitions without a full shuffle, which can be more efficient when reducing the number of partitions.

#### What was the most fascinating aspect of Spark to you while learning? 
* I guess the idea of in mem processing and the cluster idea. I understand that you can do a lot of cluster optimizitation from changing the cluster types and what sort of core theyre using. and with the new ARM processing chips its getting cheaper and faster to process data.
